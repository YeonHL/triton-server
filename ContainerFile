
#
# Multistage build.
#
FROM alpine/git AS source
WORKDIR /tmp

ARG VLLM_BACKEND_VERSION=r25.04
ARG TRITON_SERVER_VERSION=v2.56.0
ARG TRITON_CONTAINER_VERSION=25.03

RUN git clone --branch ${VLLM_BACKEND_VERSION} https://github.com/triton-inference-server/vllm_backend.git
RUN git clone --branch ${TRITON_SERVER_VERSION} https://github.com/triton-inference-server/server.git

FROM nvcr.io/nvidia/tritonserver:${TRITON_CONTAINER_VERSION}-py3 AS full
FROM nvcr.io/nvidia/tritonserver:${TRITON_CONTAINER_VERSION}-trtllm-python-py3

ARG TRITON_VERSION=2.56.0
ARG TRITON_CONTAINER_VERSION=25.03

ENV TRITON_SERVER_VERSION=${TRITON_VERSION} \
    NVIDIA_TRITON_SERVER_VERSION=${TRITON_CONTAINER_VERSION} \
    PATH=/opt/tritonserver/bin:${PATH} \
    PIP_BREAK_SYSTEM_PACKAGES=1 \
    UCX_MEM_EVENTS=no \
    TF_ADJUST_HUE_FUSED=1 \
    TF_ADJUST_SATURATION_FUSED=1 \
    TF_ENABLE_WINOGRAD_NONFUSED=1 \
    TF_AUTOTUNE_THRESHOLD=2 \
    TRITON_SERVER_GPU_ENABLED=1 \
    TRITON_SERVER_USER=triton-server \
    DEBIAN_FRONTEND=noninteractive \
    TCMALLOC_RELEASE_RATE=200 \
    DCGM_VERSION=3.3.6 \
    NVIDIA_PRODUCT_NAME="Triton Server" \
    NVIDIA_BUILD_ID=136230209

LABEL com.nvidia.tritonserver.version="${TRITON_SERVER_VERSION}" \
      com.nvidia.build.id=136230209 \
      com.nvidia.build.ref=ed3acc9acf4c657986a4e42b3dceaad3f3d9bad0 \
      com.amazonaws.sagemaker.capabilities.accept-bind-to-port=true

# Remove once https://github.com/openucx/ucx/pull/9148 is available
# in the min container.

# Create a user that can be used to run triton as
# non-root. Make sure that this user to given ID 1000. All server
# artifacts copied below are assign to this user.

# Ensure apt-get won't prompt for selecting options

# Common dependencies. FIXME (can any of these be conditional? For
# example libcurl only needed for GCS?)
WORKDIR /opt/tritonserver

RUN userdel tensorrt-server > /dev/null 2>&1 || true \
    && userdel ubuntu > /dev/null 2>&1 || true \
    && if ! id -u $TRITON_SERVER_USER > /dev/null 2>&1 ; then \
        useradd $TRITON_SERVER_USER; \
    fi \
    && [ `id -u $TRITON_SERVER_USER` -eq 1000 ] \
    && [ `id -g $TRITON_SERVER_USER` -eq 1000 ]
# Install common dependencies
RUN apt-get update \
    && apt-get install -y --no-install-recommends \
        clang \
        curl \
        datacenter-gpu-manager=1:3.3.6 \
        dirmngr \
        gperf \
        libb64-0d \
        libcurl4-openssl-dev \
        libgl1 \
        libglib2.0-0 \
        libgoogle-perftools-dev \
        libjemalloc-dev \
        libjpeg-dev \
        libnuma-dev \
        python3-pip \
        software-properties-common \
        wget \
        zlib1g-dev \
      && rm -rf /var/lib/apt/lists/* \
      && rm -f /tmp/cuda-keyring.deb

COPY --chown=1000:1000 requirements.txt .

# Install Python dependencies
RUN pip install --no-cache-dir -r requirements.txt \
    && rm -rf ~/.cache/pip

# Extra defensive wiring for CUDA Compat lib
RUN ln -sf ${_CUDA_COMPAT_PATH}/lib.real ${_CUDA_COMPAT_PATH}/lib \
      && echo ${_CUDA_COMPAT_PATH}/lib > /etc/ld.so.conf.d/00-cuda-compat.conf \
      && ldconfig \
      && rm -f ${_CUDA_COMPAT_PATH}/lib \
      # Create directories with proper permissions
      && mkdir -p /opt/tritonserver/repoagents \
      && install -d -m 777 -o $TRITON_SERVER_USER -g nogroup /models \
      && install -d -m 777 -o $TRITON_SERVER_USER -g nogroup /home/$TRITON_SERVER_USER/.cache/huggingface/hub \
      && touch /home/$TRITON_SERVER_USER/.cache/huggingface/stored_tokens \
      && chown -R $TRITON_SERVER_USER:nogroup /home/$TRITON_SERVER_USER \
      && chmod -R 777 /home/$TRITON_SERVER_USER/.cache/huggingface \
      && chown -R $TRITON_SERVER_USER:$TRITON_SERVER_USER /opt/tritonserver

# Extra defensive wiring for CUDA Compat lib
COPY --chown=1000:1000 --from=full /opt/tritonserver/LICENSE \
                                    /opt/tritonserver/TRITON_VERSION \
                                    /opt/tritonserver/NVIDIA_Deep_Learning_Container_License.pdf \
                                    ./
COPY --chown=1000:1000 --from=full /opt/tritonserver/bin bin/
COPY --chown=1000:1000 --from=full /opt/tritonserver/lib lib/
COPY --chown=1000:1000 --from=full /opt/tritonserver/include include/
COPY --chown=1000:1000 --from=full /opt/tritonserver/backends backends/
COPY --chown=1000:1000 --from=full /opt/tritonserver/repoagents/checksum /opt/tritonserver/repoagents/checksum
COPY --chown=1000:1000 --from=full /usr/bin/serve /usr/bin/

# Copy Github repositories
COPY --chown=1000:1000 --from=source /tmp/vllm_backend/src /opt/tritonserver/backends/vllm
COPY --chown=1000:1000 --from=source /tmp/server/docker/entrypoint.d /opt/nvidia/entrypoint.d

USER $TRITON_SERVER_USER

VOLUME ["/models", "/home/$TRITON_SERVER_USER/.cache/huggingface/hub"]
EXPOSE 8000 8001 8002
ENTRYPOINT ["tritonserver"]
CMD ["--model-control-mode", "explicit", "--model-repository", "/models"]